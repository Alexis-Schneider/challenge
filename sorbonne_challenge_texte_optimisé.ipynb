{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d1a9d0",
   "metadata": {},
   "source": [
    "Séquentialisation des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ff016",
   "metadata": {},
   "source": [
    "Importation du fichier .csv adapté pour le nombre de fichier avec lesquels on travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv('training_set_metadata.csv', sep=';')\n",
    "folder_path = \"destination\"\n",
    "\n",
    "# Vérifier les fichiers JSON\n",
    "missing_files = []\n",
    "for idx, row in df.iterrows():\n",
    "    json_path = os.path.join(folder_path, f\"{row['name']}.json\")\n",
    "    if not os.path.exists(json_path):\n",
    "        missing_files.append(row['name'])\n",
    "\n",
    "# Afficher les fichiers manquants\n",
    "if missing_files:\n",
    "    print(\"Fichiers manquants :\")\n",
    "    for file in missing_files:\n",
    "        print(f\"- {file}\")\n",
    "else:\n",
    "    print(\"Tous les fichiers sont présents.\")\n",
    "\n",
    "# Optionnel : Mettre à jour le CSV pour ne conserver que les fichiers existants\n",
    "if missing_files:\n",
    "    df_updated = df[~df['name'].isin(missing_files)]\n",
    "    df_updated.to_csv(\"your_data_updated.csv\", index=False)\n",
    "    print(\"Fichier CSV mis à jour pour ne conserver que les fichiers existants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6496a",
   "metadata": {},
   "source": [
    "(Optionel: permet de déplacer les fichiers .json dans un autre répertoir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de79a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_json_files(source_directory, destination_directory):\n",
    "    # Créer le répertoire de destination s'il n'existe pas\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Parcourir tous les fichiers dans le répertoire source\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            # Vérifier si le fichier est un fichier de mapping ou de matrice d'adjacence\n",
    "            if \"_assembly_mapping.json\" in filename or \"_adj_matrix.npz\" in filename:\n",
    "                continue  # Ignorer ces fichiers\n",
    "\n",
    "            # Déplacer le fichier .json vers le répertoire de destination\n",
    "            shutil.move(os.path.join(source_directory, filename), os.path.join(destination_directory, filename))\n",
    "            print(f\"Déplacé : {filename}\")\n",
    "\n",
    "# Utilisation des dossiers \"graphes\" et \"destination\"\n",
    "source_directory = \"graphes\"\n",
    "destination_directory = \"destination\"\n",
    "move_json_files(source_directory, destination_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485ac7c",
   "metadata": {},
   "source": [
    "Extraction du texte + vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a366683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.sparse import vstack, save_npz, load_npz\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Expressions régulières pour extraire arêtes et nœuds\n",
    "EDGE_REGEX = re.compile(r'\"([a-zA-Z0-9]+)\"\\s*->\\s*\"([a-zA-Z0-9]+)\"')\n",
    "NODE_REGEX = re.compile(r'\"([a-zA-Z0-9]+)\"\\s*\\[label\\s*=\\s*\"(.*?)\"\\s*\\]')\n",
    "\n",
    "def parse_cfg_file(cfg_path):\n",
    "    try:\n",
    "        with open(cfg_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier {cfg_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_file(name, directory):\n",
    "    \"\"\"\n",
    "    Lit un fichier JSON, construit un graphe avec NetworkX et extrait les instructions via DFS.\n",
    "    Retourne une liste d'instructions (chaînes de caractères) ou None en cas d'erreur.\n",
    "    \"\"\"\n",
    "    cfg_path = os.path.join(directory, f\"{name}.json\")\n",
    "    if not os.path.exists(cfg_path):\n",
    "        print(f\"Le fichier {cfg_path} n'existe pas.\")\n",
    "        return None\n",
    "    try:\n",
    "        G = nx.DiGraph()\n",
    "        lines = parse_cfg_file(cfg_path)\n",
    "        if not lines:\n",
    "            print(f\"Le fichier {cfg_path} est vide ou n'a pas pu être lu.\")\n",
    "            return None\n",
    "\n",
    "        # Extraction des arêtes et des nœuds\n",
    "        for line in lines:\n",
    "            if '->' in line:\n",
    "                edge_match = EDGE_REGEX.match(line)\n",
    "                if edge_match:\n",
    "                    src, dst = edge_match.groups()\n",
    "                    G.add_edge(src, dst)\n",
    "            if \"[\" in line and \"]\" in line:\n",
    "                node_match = NODE_REGEX.match(line)\n",
    "                if node_match:\n",
    "                    node, label = node_match.groups()\n",
    "                    G.add_node(node, label=label)\n",
    "\n",
    "        # Extraction des instructions via DFS\n",
    "        instructions = []\n",
    "        for node in nx.dfs_preorder_nodes(G):\n",
    "            if 'label' in G.nodes[node]:\n",
    "                instructions.append(G.nodes[node]['label'])\n",
    "        print(f\"Fichier {name}.json: {G.number_of_nodes()} nœuds, {G.number_of_edges()} arêtes.\")\n",
    "        return instructions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de {cfg_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def batched(iterable, batch_size):\n",
    "    \"\"\"Générateur qui découpe un itérable en lots de taille batch_size.\"\"\"\n",
    "    batch = []\n",
    "    for item in iterable:\n",
    "        batch.append(item)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def process_graphs_in_directory(directory, batch_size=100):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "    partial_files = []  # Pour enregistrer le nom des fichiers de batch\n",
    "    batch_index = 0\n",
    "\n",
    "    # Chargement du modèle SpaCy (si besoin pour d'autres traitements)\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Traitement par lots avec barre de progression\n",
    "    for batch_files in tqdm(list(batched(files, batch_size)), desc=\"Traitement par batch\"):\n",
    "        docs = []  # Liste des documents (texte brut) du batch\n",
    "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            futures = {executor.submit(process_file, os.path.splitext(filename)[0], directory): filename \n",
    "                       for filename in batch_files}\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        docs.append(\" \".join(result))\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du traitement d'un fichier: {e}\")\n",
    "        gc.collect()\n",
    "\n",
    "        if docs:\n",
    "            # Utilisation du HashingVectorizer pour obtenir une matrice TF du batch\n",
    "            hv = HashingVectorizer(n_features=2**20, alternate_sign=False, norm=None)\n",
    "            batch_matrix = hv.transform(docs)\n",
    "            # Enregistrement du batch sur disque\n",
    "            batch_file = os.path.join(directory, f\"tf_batch_{batch_index}.npz\")\n",
    "            save_npz(batch_file, batch_matrix)\n",
    "            partial_files.append(batch_file)\n",
    "            print(f\"Batch {batch_index} enregistré : forme {batch_matrix.shape}\")\n",
    "            batch_index += 1\n",
    "\n",
    "    # Si au moins un batch a été traité, on combine toutes les matrices\n",
    "    if partial_files:\n",
    "        matrices = [load_npz(f) for f in partial_files]\n",
    "        full_tf_matrix = vstack(matrices)\n",
    "        transformer = TfidfTransformer()\n",
    "        tfidf_matrix = transformer.fit_transform(full_tf_matrix)\n",
    "        final_file = os.path.join(directory, \"tfidf_matrix.npz\")\n",
    "        save_npz(final_file, tfidf_matrix)\n",
    "        print(\"Matrice TF-IDF finale de forme :\", tfidf_matrix.shape)\n",
    "    else:\n",
    "        print(\"Aucun document traité.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    directory = \"destination\"\n",
    "    process_graphs_in_directory(directory, batch_size=100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5b27e",
   "metadata": {},
   "source": [
    "Modèle de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fd115",
   "metadata": {},
   "source": [
    "Modele MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd28666",
   "metadata": {},
   "source": [
    "Importation des bibliotheques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182b1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, issparse, csr_matrix, save_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9667972",
   "metadata": {},
   "source": [
    "Chargement et Prétraitement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaed36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charger_donnees(csv_path='your_data_updated.csv', matrix_path='destination/tfidf_matrix.npz', variance_threshold=0.95):\n",
    "    \"\"\"Charge les données depuis un fichier CSV et une matrice TF-IDF, applique Truncated SVD.\"\"\"\n",
    "    try:\n",
    "        # Charger le fichier CSV\n",
    "        df = pd.read_csv(csv_path, sep=';')\n",
    "\n",
    "        # Charger la matrice TF-IDF\n",
    "        X = load_npz(matrix_path)\n",
    "        print(\"Matrice TF-IDF chargée de forme :\", X.shape)\n",
    "\n",
    "        # Convertir en matrice creuse si ce n'est pas le cas\n",
    "        if not issparse(X):\n",
    "            X = csr_matrix(X)\n",
    "\n",
    "        # Réduction de dimensionnalité avec Truncated SVD\n",
    "        svd = TruncatedSVD(n_components=min(X.shape[1], 300))\n",
    "        X_reduced = svd.fit_transform(X)\n",
    "\n",
    "        # Calculer la variance expliquée\n",
    "        explained_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "        print(\"Variance expliquée pour les 10 premières dimensions :\", explained_variance[:10])\n",
    "\n",
    "        # Sélectionner le nombre de composants qui capture au moins 'variance_threshold' de la variance\n",
    "        n_components = np.argmax(explained_variance >= variance_threshold) + 1\n",
    "        print(f\"Nombre de composants sélectionnés pour capturer {variance_threshold*100}% de la variance : {n_components}\")\n",
    "\n",
    "        # Réduire à nouveau la matrice avec le nombre optimal de composants\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        X_reduced = svd.fit_transform(X)\n",
    "        print(\"Matrice TF-IDF réduite de forme :\", X_reduced.shape)\n",
    "\n",
    "        # Enregistrer la matrice creuse\n",
    "        save_npz('reduced_tfidf_matrix.npz', csr_matrix(X_reduced))\n",
    "\n",
    "        # Extraire les colonnes cibles\n",
    "        colonnes_cibles = df.columns.difference(['name'])\n",
    "        y = df[colonnes_cibles].values\n",
    "\n",
    "        # Binariser les cibles multi-labels\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        y_binarized = mlb.fit_transform(y)\n",
    "        print(\"Cibles binarisées de forme :\", y_binarized.shape)\n",
    "\n",
    "        # Aligner les données\n",
    "        if X_reduced.shape[0] != y_binarized.shape[0]:\n",
    "            print(f\"Alignement des données : Suppression de {X_reduced.shape[0] - y_binarized.shape[0]} lignes de la matrice TF-IDF.\")\n",
    "            X_reduced = X_reduced[:y_binarized.shape[0]]\n",
    "\n",
    "        # Appliquer SMOTE pour équilibrer les classes\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_reduced, y_binarized)\n",
    "\n",
    "        return train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données : {e}\")\n",
    "        return None\n",
    "\n",
    "# Exemple d'utilisation\n",
    "donnees = charger_donnees()\n",
    "if donnees:\n",
    "    X_train, X_test, y_train, y_test = donnees\n",
    "    print(\"Données chargées et traitées avec succès.\")\n",
    "else:\n",
    "    print(\"Échec du chargement des données.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f22056",
   "metadata": {},
   "source": [
    "Visualisation du fort désequilibre entre les 0 et 1 dans le fichier .csv et utilisation de SMOTE(pas optimal) pour gérer ce problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93670ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv('your_data_updated.csv', sep=',')\n",
    "print(\"CSV chargé avec succès. Forme :\", df.shape)\n",
    "print(\"Colonnes :\", df.columns.tolist())\n",
    "\n",
    "# Vérifier que la colonne 'name' est présente\n",
    "if 'name' not in df.columns:\n",
    "    raise ValueError(\"La colonne 'name' est introuvable dans le CSV.\")\n",
    "\n",
    "# Extraire les cibles : toutes les colonnes sauf 'name'\n",
    "y_df = df.drop(columns=['name'])\n",
    "print(\"Cibles extraites de forme :\", y_df.shape)\n",
    "\n",
    "# Afficher la répartition initiale des classes pour chaque colonne\n",
    "print(\"Répartition des classes avant suréchantillonnage :\")\n",
    "for col in y_df.columns:\n",
    "    counts = y_df[col].value_counts()\n",
    "    total = counts.sum()\n",
    "    print(f\"Colonne '{col}':\")\n",
    "    print(counts)\n",
    "    for valeur, count in counts.items():\n",
    "        pourcentage = count / total * 100\n",
    "        print(f\"  Valeur {valeur} : {count} ({pourcentage:.2f}%)\")\n",
    "    print()\n",
    "\n",
    "# Créer un X factice, car SMOTE a besoin de features\n",
    "X = np.ones((y_df.shape[0], 1))\n",
    "\n",
    "# Dictionnaire pour stocker les résultats suréchantillonnés pour chaque colonne\n",
    "y_res_dict = {}\n",
    "\n",
    "print(\"Suréchantillonnage par colonne avec RandomOverSampler :\")\n",
    "for col in y_df.columns:\n",
    "    # Si la colonne ne contient qu'une seule classe, on l'ignore\n",
    "    if y_df[col].nunique() < 2:\n",
    "        print(f\"La colonne '{col}' contient une seule classe. Elle sera ignorée.\")\n",
    "        continue\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y_df[col])\n",
    "    y_res_dict[col] = y_res\n",
    "    print(f\"Après suréchantillonnage pour '{col}' : {np.bincount(y_res)}\")\n",
    "    total = np.bincount(y_res).sum()\n",
    "    for valeur, count in enumerate(np.bincount(y_res)):\n",
    "        pourcentage = count / total * 100\n",
    "        print(f\"  Valeur {valeur} : {count} ({pourcentage:.2f}%)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5f44c",
   "metadata": {},
   "source": [
    "Création du Modèle MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04513e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import load_npz, issparse, csr_matrix, save_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# -------------------------\n",
    "# 1. Fonction de perte Focal Loss\n",
    "# -------------------------\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = 1e-7\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * tf.math.pow(1 - y_pred, gamma)\n",
    "        loss = weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# -------------------------\n",
    "# 2. Analyse de la distribution des labels\n",
    "# -------------------------\n",
    "def compute_label_distribution(y):\n",
    "    label_freq = np.sum(y, axis=0)\n",
    "    print(\"Distribution des labels (occurrences par label) :\", label_freq)\n",
    "    print(\"Pourcentage moyen d'occurrences par label :\", np.mean(label_freq / y.shape[0]))\n",
    "    return label_freq\n",
    "\n",
    "# -------------------------\n",
    "# 3. Chargement et prétraitement des données\n",
    "# -------------------------\n",
    "def charger_donnees(csv_path='your_data_updated.csv', \n",
    "                    reduced_matrix_path='reduced_tfidf_matrix.npz', \n",
    "                    apply_svd=False, \n",
    "                    variance_threshold=0.95, \n",
    "                    use_smote=False):\n",
    "    \"\"\"\n",
    "    Charge le CSV et la matrice TF-IDF.\n",
    "      - apply_svd: si True, applique une réduction par Truncated SVD en fonction du seuil de variance.\n",
    "      - use_smote: si True, applique SMOTE (attention: SMOTE standard n'est pas optimal pour le multi-label).\n",
    "    Les cibles sont toutes les colonnes du CSV sauf 'name'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger le CSV \n",
    "        df = pd.read_csv(csv_path, sep=',')\n",
    "        print(\"CSV chargé avec succès. Forme :\", df.shape)\n",
    "        print(\"Colonnes :\", df.columns.tolist())\n",
    "        \n",
    "        # Charger la matrice TF-IDF (ou la matrice réduite)\n",
    "        X = load_npz(reduced_matrix_path)\n",
    "        print(\"Matrice TF-IDF chargée de forme :\", X.shape)\n",
    "        if not issparse(X):\n",
    "            X = csr_matrix(X)\n",
    "            \n",
    "        # Si demandé, appliquer SVD pour réduire la dimension\n",
    "        if apply_svd:\n",
    "            svd = TruncatedSVD(n_components=min(X.shape[1], 300))\n",
    "            X_temp = svd.fit_transform(X)\n",
    "            explained_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "            n_components = np.argmax(explained_variance >= variance_threshold) + 1\n",
    "            print(f\"Réduction SVD: {n_components} composantes pour capturer {variance_threshold*100}% de la variance.\")\n",
    "            svd = TruncatedSVD(n_components=n_components)\n",
    "            X_reduced = svd.fit_transform(X)\n",
    "            \n",
    "        else:\n",
    "            X_reduced = X\n",
    "            \n",
    "        # Extraire les cibles : toutes les colonnes sauf 'name'\n",
    "        if 'name' not in df.columns:\n",
    "            raise ValueError(\"La colonne 'name' est introuvable dans le CSV.\")\n",
    "        y = df.drop(columns=['name']).values\n",
    "        print(\"Cibles extraites de forme :\", y.shape)\n",
    "        \n",
    "        # Afficher la distribution des labels\n",
    "        compute_label_distribution(y)\n",
    "        \n",
    "        # Aligner les données si nécessaire\n",
    "        if X_reduced.shape[0] != y.shape[0]:\n",
    "            diff = X_reduced.shape[0] - y.shape[0]\n",
    "            print(f\"Alignement: suppression de {diff} lignes de la matrice TF-IDF.\")\n",
    "            X_reduced = X_reduced[:y.shape[0]]\n",
    "        \n",
    "        # Idée: SMOTE (par défaut désactivé pour le multi-label car fonctionne pas bien)\n",
    "        if use_smote:\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_reduced, y)\n",
    "        else:\n",
    "            X_resampled, y_resampled = X_reduced, y\n",
    "        \n",
    "        return train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Erreur lors du chargement des données :\", e)\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# 4. Création du modèle MLP\n",
    "# -------------------------\n",
    "def creer_modele_mlp_optimise(input_dim, output_dim, use_focal_loss=False):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "\n",
    "    # Ajustement des couches\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "    if use_focal_loss:\n",
    "        loss_fn = focal_loss(gamma=2., alpha=0.25)\n",
    "    else:\n",
    "        loss_fn = 'binary_crossentropy'\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=loss_fn, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Entraîner et évaluer le modèle optimisé\n",
    "donnees = charger_donnees(csv_path='your_data_updated.csv',\n",
    "                          reduced_matrix_path='reduced_tfidf_matrix.npz',\n",
    "                          apply_svd=True,\n",
    "                          variance_threshold=0.95,\n",
    "                          use_smote=False)\n",
    "\n",
    "if donnees:\n",
    "    X_train, X_test, y_train, y_test = donnees\n",
    "    modele_optimise = entrainer_et_evaluer_modele(X_train, y_train, X_test, y_test, use_focal_loss=True)\n",
    "else:\n",
    "    print(\"Échec du chargement des données.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Visualisation et entraînement\n",
    "# -------------------------\n",
    "def visualiser_resultats(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Évolution de la loss pendant l'entraînement\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def entrainer_et_evaluer_modele(X_train, y_train, X_test, y_test, use_focal_loss=False):\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "    print(\"Dimension d'entrée :\", input_dim, \"Dimension de sortie :\", output_dim)\n",
    "    \n",
    "    modele = creer_modele_mlp(input_dim, output_dim, use_focal_loss=use_focal_loss)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = modele.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2,\n",
    "                         callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    visualiser_resultats(history)\n",
    "    \n",
    "    y_pred = (modele.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    score = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    print(\"Score F1 macro :\", score)\n",
    "    return modele\n",
    "\n",
    "# -------------------------\n",
    "# Exemple d'utilisation\n",
    "# -------------------------\n",
    "donnees = charger_donnees(csv_path='your_data_updated.csv', \n",
    "                          reduced_matrix_path='reduced_tfidf_matrix.npz', \n",
    "                          apply_svd=False,    \n",
    "                          variance_threshold=0.95, \n",
    "                          use_smote=False)     # SMOTE est désactivé par défaut pour le multi-label\n",
    "\n",
    "if donnees:\n",
    "    X_train, X_test, y_train, y_test = donnees\n",
    "    modele = entrainer_et_evaluer_modele(X_train, y_train, X_test, y_test, use_focal_loss=True)\n",
    "else:\n",
    "    print(\"Échec du chargement des données.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7798d3",
   "metadata": {},
   "source": [
    "Test autre model : LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.sparse import vstack, save_npz, load_npz\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Expressions régulières pour extraire arêtes et nœuds\n",
    "EDGE_REGEX = re.compile(r'\"([a-zA-Z0-9]+)\"\\s*->\\s*\"([a-zA-Z0-9]+)\"')\n",
    "NODE_REGEX = re.compile(r'\"([a-zA-Z0-9]+)\"\\s*\\[label\\s*=\\s*\"(.*?)\"\\s*\\]')\n",
    "\n",
    "def parse_cfg_file(cfg_path):\n",
    "    try:\n",
    "        with open(cfg_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier {cfg_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_file(name, directory):\n",
    "    cfg_path = os.path.join(directory, f\"{name}.json\")\n",
    "    if not os.path.exists(cfg_path):\n",
    "        return None\n",
    "    try:\n",
    "        G = nx.DiGraph()\n",
    "        lines = parse_cfg_file(cfg_path)\n",
    "        if not lines:\n",
    "            return None\n",
    "\n",
    "        node_labels = {}\n",
    "        for line in lines:\n",
    "            if '->' in line:\n",
    "                if edge_match := EDGE_REGEX.match(line):\n",
    "                    G.add_edge(*edge_match.groups())\n",
    "            elif node_match := NODE_REGEX.search(line):\n",
    "                node, label = node_match.groups()\n",
    "                node_labels[node] = label\n",
    "\n",
    "        # Ajout des labels après création des noeuds\n",
    "        nx.set_node_attributes(G, node_labels, name='label')\n",
    "        \n",
    "        # Tri topologique pour l'ordre des instructions\n",
    "        try:\n",
    "            ordered_nodes = list(nx.topological_sort(G))\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            ordered_nodes = list(G.nodes)\n",
    "            \n",
    "        instructions = [G.nodes[node].get('label', '') for node in ordered_nodes]\n",
    "        return ' '.join(instructions[:200])  # Limite de séquence\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur dans {cfg_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_graphs_in_directory(directory, batch_size=100):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "    final_file = os.path.join(directory, \"tokenized_matrix.npz\")\n",
    "    \n",
    "    if os.path.exists(final_file):\n",
    "        return load_npz(final_file)\n",
    "    \n",
    "    vectorizer = HashingVectorizer(n_features=2**18, alternate_sign=False, ngram_range=(1, 2))\n",
    "    partial_files = []\n",
    "\n",
    "    for batch_idx, batch_files in enumerate(tqdm(batched(files, batch_size), desc=\"Processing batches\")):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_file, os.path.splitext(f)[0], directory) for f in batch_files]\n",
    "            docs = [f.result() for f in tqdm(as_completed(futures), desc=f\"Batch {batch_idx}\") if f.result()]\n",
    "\n",
    "        if docs:\n",
    "            X_batch = vectorizer.transform(docs)\n",
    "            batch_path = os.path.join(directory, f\"tokenized_batch_{batch_idx}.npz\")\n",
    "            save_npz(batch_path, X_batch)\n",
    "            partial_files.append(batch_path)\n",
    "\n",
    "    # Combinaison des batches\n",
    "    X_full = vstack([load_npz(f) for f in partial_files])\n",
    "    save_npz(final_file, X_full)\n",
    "    return X_full\n",
    "\n",
    "def create_lstm_model(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(64, return_sequences=True, input_shape=(1, input_dim))),\n",
    "        Dropout(0.5),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(output_dim, activation='sigmoid' if output_dim == 1 else 'softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy' if output_dim == 1 else 'categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Pipeline principal\n",
    "if __name__ == \"__main__\":\n",
    "    directory = \"destination\"\n",
    "    X = process_graphs_in_directory(directory)\n",
    "    \n",
    "    # Chargement des labels\n",
    "    df = pd.read_csv('your_data_updated.csv')\n",
    "    y = df.drop(columns=['name']).values\n",
    "    \n",
    "    # Réduction de dimension\n",
    "    svd = TruncatedSVD(n_components=300)\n",
    "    X_reduced = svd.fit_transform(X)\n",
    "    \n",
    "    # Alignement des données\n",
    "    if X_reduced.shape[0] > y.shape[0]:\n",
    "        X_reduced = X_reduced[:y.shape[0]]\n",
    "    else:\n",
    "        y = y[:X_reduced.shape[0]]\n",
    "    \n",
    "    # Split des données\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reduced, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Reshape pour LSTM [échantillons, pas de temps, caractéristiques]\n",
    "    X_train = X_train.reshape((*X_train.shape, 1))\n",
    "    X_test = X_test.reshape((*X_test.shape, 1))\n",
    "    \n",
    "    # Création et entraînement du modèle\n",
    "    model = create_lstm_model(X_train.shape[2], y_train.shape[1])\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Évaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int) if y_train.shape[1] == 1 else np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    f1 = f1_score(\n",
    "        y_test.argmax(axis=1) if y_train.shape[1] > 1 else y_test,\n",
    "        y_pred_classes,\n",
    "        average='macro',\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e1aa1",
   "metadata": {},
   "source": [
    "Modele classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c65c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import MultiLabelSMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def charger_donnees(csv_path, matrix_path):\n",
    "    \"\"\"Charge les données depuis un fichier CSV et une matrice TF-IDF.\"\"\"\n",
    "    try:\n",
    "        # Charger le fichier CSV\n",
    "        df = pd.read_csv(csv_path, sep=';')\n",
    "\n",
    "        # Charger la matrice TF-IDF\n",
    "        X = load_npz(matrix_path)\n",
    "        print(\"Matrice TF-IDF chargée de forme :\", X.shape)\n",
    "\n",
    "        # Extraire les colonnes cibles\n",
    "        colonnes_cibles = df.columns.difference(['name'])\n",
    "        y = df[colonnes_cibles].values\n",
    "\n",
    "        # Vérifier si les cibles sont déjà binarisées (one-hot encoded)\n",
    "        if not ((y == 0) | (y == 1)).all():\n",
    "            # Binariser les cibles multi-étiquettes si nécessaire\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            y = mlb.fit_transform(y)\n",
    "            print(\"Cibles binarisées de forme :\", y.shape)\n",
    "        else:\n",
    "            print(\"Cibles déjà binarisées de forme :\", y.shape)\n",
    "\n",
    "        # Aligner les données\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            print(f\"Alignement des données : Suppression de {X.shape[0] - y.shape[0]} lignes de la matrice TF-IDF.\")\n",
    "            X = X[:y.shape[0]]\n",
    "\n",
    "        # Appliquer MultiLabelSMOTE pour équilibrer les classes\n",
    "        ml_smote = MultiLabelSMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = ml_smote.fit_resample(X, y)\n",
    "        print(\"Données rééchantillonnées de forme :\", X_resampled.shape, y_resampled.shape)\n",
    "\n",
    "        return train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données : {e}\")\n",
    "        return None\n",
    "\n",
    "def entrainer_et_evaluer_modele(modele, grille_param, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Entraîne un modèle en utilisant GridSearchCV et évalue ses performances.\"\"\"\n",
    "    f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "    recherche_grille = GridSearchCV(estimator=OneVsRestClassifier(modele), param_grid=grille_param,\n",
    "                                    scoring=f1_scorer, cv=3, verbose=1, n_jobs=-1)\n",
    "    recherche_grille.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Meilleurs paramètres pour {modele.__class__.__name__} :\", recherche_grille.best_params_)\n",
    "    print(f\"Meilleur score F1 pour {modele.__class__.__name__} :\", recherche_grille.best_score_)\n",
    "\n",
    "    # Visualisation des résultats\n",
    "    visualiser_resultats(recherche_grille, modele.__class__.__name__)\n",
    "\n",
    "    y_pred = recherche_grille.predict(X_test)\n",
    "    print(f\"Score F1 macro pour {modele.__class__.__name__} :\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "def visualiser_resultats(recherche_grille, nom_modele):\n",
    "    \"\"\"Visualise les résultats de la recherche des hyperparamètres.\"\"\"\n",
    "    results = recherche_grille.cv_results_\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['mean_test_score'], label='Score moyen')\n",
    "    plt.xlabel('Index de l\\'itération')\n",
    "    plt.ylabel('Score F1 Macro')\n",
    "    plt.title(f'Évolution du score F1 Macro pour {nom_modele}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def principal():\n",
    "    # Définir les chemins des fichiers\n",
    "    csv_path = 'your_data_updated.csv'\n",
    "    matrix_path = 'destination/tfidf_matrix.npz'\n",
    "\n",
    "    # Charger les données\n",
    "    donnees = charger_donnees(csv_path, matrix_path)\n",
    "    if donnees:\n",
    "        X_train, X_test, y_train, y_test = donnees\n",
    "\n",
    "        # Définir les grilles de paramètres\n",
    "        grille_param_lgb = {\n",
    "            'estimator__n_estimators': [100, 200],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__num_leaves': [31, 50]\n",
    "        }\n",
    "\n",
    "        grille_param_xgb = {\n",
    "            'estimator__n_estimators': [100, 200],\n",
    "            'estimator__learning_rate': [0.01, 0.1],\n",
    "            'estimator__max_depth': [3, 5]\n",
    "        }\n",
    "\n",
    "        # Initialiser les modèles\n",
    "        modele_lgb = lgb.LGBMClassifier()\n",
    "        modele_xgb = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "        # Entraîner et évaluer les modèles\n",
    "        entrainer_et_evaluer_modele(modele_lgb, grille_param_lgb, X_train, y_train, X_test, y_test)\n",
    "        entrainer_et_evaluer_modele(modele_xgb, grille_param_xgb, X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        print(\"Échec du chargement des données.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "::contentReference[oaicite:10]{index=10}\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
